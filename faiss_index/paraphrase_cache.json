{
  "Title: Evaluation Metrics — Accuracy, Precision, Recall, F1, and ROC-AUC Abstract: This document provides practical guidance on choosing and interpreting classification metrics, especially under class imbalance and varying decision thresholds.": "Title: Evaluation Metrics — Accuracy, Precision, Recall, F1, and ROC-AUC Abstract: This document provides practical guidance on choosing and interpreting classification metrics, especially under class imbalance and varying decision thresholds.",
  "Extended Topics and Practical Guidance: 1) Precision–Recall (PR) Curves and PR-AUC - Better than ROC when positives are rare; focuses on performance on the positive class.": "Extended Topics and Practical Guidance: 1) Precision–Recall (PR) Curves and PR-AUC - Better than ROC when positives are rare; focuses on performance on the positive class.",
  "4) ROC and AUC - ROC curve: TPR vs.": "4) ROC and AUC - ROC curve: TPR vs.",
  "- Use curves (ROC/PR) plus thresholded metrics; report per-class results.": "- Use curves (ROC/PR) plus thresholded metrics; report per-class results.",
  "FPR across thresholds; AUC is threshold-agnostic ranking quality.": "FPR across thresholds; AUC is threshold-agnostic ranking quality.",
  "- Search ranking: AUC/PR-AUC and calibration for user experience.": "- Search ranking: AUC/PR-AUC and calibration for user experience.",
  "- ROC curve: TPR vs.": "- ROC curve: TPR vs.",
  "Use curves (ROC/PR), not just point metrics, and analyze calibration when predictions inform downstream decisions.": "Use curves (ROC/PR), not just point metrics, and analyze calibration when predictions inform downstream decisions.",
  "- PR-AUC summarizes across thresholds; inspect curve shape for head vs.": "- - PR-AUC summarizes across thresholds; inspect curve shape for head vs. - Conclusion: Choose metrics aligned with costs of false positives/negatives and operational use.",
  "Conclusion: Choose metrics aligned with costs of false positives/negatives and operational use.": "Conclusion: Choose metrics aligned with costs of false positives/negatives and operational use.",
  "5) Calibration and Other Metrics - Brier score, log loss: Assess probability quality, not just ranking.": "5) Calibration and Other Metrics - Brier score, log loss: Assess probability quality, not just ranking.",
  "Accuracy alone can be misleading; precision–recall trade-offs matter in many domains.": "Accuracy alone can be misleading; precision–recall trade-offs matter in many domains.",
  "2) Embeddings - Static: word2vec, GloVe; one vector per token regardless of context.": "Embeddings - Static: word2vec, GloVe; one vector per token regardless of context.",
  "- Contextual: ELMo, BERT embeddings; token meaning varies with sentence context.": "- Contextual: ELMo, BERT embeddings; token meaning varies with sentence context.",
  "Conclusion: Tokenization and embeddings underpin NLP pipelines,": "Conclusion: Tokenization and embeddings underpin NLP pipelines,",
  "Title: Natural Language Processing — Tokenization, Embeddings, BERT, GPT, and T5 Abstract: This document outlines key components of modern NLP: tokenization schemas, representation learning via embeddings, and transformer-based model families BERT, GPT, and T5.": "Title: Natural Language Processing — Tokenization, Embeddings, BERT, GPT, and T5 Abstract: This document outlines key components of modern NLP: tokenization schemas, representation learning via embeddings, and transformer-based model families BERT, GPT, and T5.",
  "2) Embeddings Best Practices - Domain Adaptation: Further pretrain sentence embedding models on in-domain corpora for retrieval tasks.": "2) Embeddings Best Practices - Domain Adaptation: Further pretrain sentence embedding models on in-domain corpora for retrieval tasks.",
  "Conclusion: Tokenization and embeddings underpin NLP pipelines, while transformer families provide complementary strengths: BERT for understanding, GPT for generation, and T5 for task-unified text-to-text formulations.": "Conclusion: Tokenization and embeddings underpin NLP pipelines, while transformer families provide complementary strengths: BERT for understanding, GPT for generation, and T5 for task-unified text-to-text formulations.",
  "- Sentence/document embeddings: SBERT, Universal Sentence Encoder for retrieval and clustering.": "- Sentence/document embeddings: SBERT, Universal Sentence Encoder for retrieval and clustering.",
  "Applications: - Tokenization/Embeddings: Search, clustering, semantic similarity.": "Applications: - Tokenization/Embeddings: Search, clustering, semantic similarity.",
  "Modern NLP systems benefit from PEFT techniques, RAG for grounding, robust evaluation, and safety mitigations.": "6) Multilingual and Cross-lingual NLP - Multilingual Models: mBERT, XLM-R; benefit from shared subword vocabularies. 3) BERT (Encoder-only) - Pretraining: Masked Language Modeling (MLM) + Next Sentence Prediction (NSP)",
  "6) Multilingual and Cross-lingual NLP - Multilingual Models: mBERT, XLM-R; benefit from shared subword vocabularies.": "6) Multilingual and Cross-lingual NLP - Multilingual Models: mBERT, XLM-R; benefit from shared subword vocabularies.",
  "3) BERT (Encoder-only) - Pretraining: Masked Language Modeling (MLM) + Next Sentence Prediction (NSP, often removed in newer variants).": "3) BERT (Encoder-only) - Pretraining: Masked Language Modeling (MLM) + Next Sentence Prediction (NSP, often removed in newer variants).",
  "Introduction: NLP systems transform raw text into structured representations suitable for machine learning.": "Introduction: NLP systems transform raw text into structured representations suitable for machine learning.",
  "- Use Cases: Early NLP, speech, time series forecasting.": "Use Cases: NLP, speech, time series forecasting.",
  "- Use Cases: NLP, vision (ViT, DETR), speech, genomics, multimodal tasks.": "- Use Cases: NLP, vision (ViT, DETR), speech, genomics, multimodal tasks.",
  "- Use Cases: Language modeling, sequence tagging, multivariate time series.": "- Use Cases: Language modeling, sequence tagging, multivariate time series.",
  "Core Concepts / Methods: 1) Tokenization - Word-level: Simple but suffers from OOV (out-of-vocabulary) issues.": "Core Concepts / Methods: 1) Tokenization - Word-level: Simple but suffers from OOV (out-of-vocabulary) issues.",
  "Masked Language Modeling (MLM) + Next Sentence Prediction (NSP, often removed in newer variants).": "Masked Language Modeling (MLM) + Next Sentence Prediction (NSP, often removed in newer variants).",
  "- Usage: Strong for understanding tasks (classification, QA with span extraction).": "- Usage: Strong for understanding tasks (classification, QA with span extraction).",
  "Core Concepts / Methods: 1) CNNs (Convolutional Neural Networks) - Mechanism: Convolutions exploit spatial locality and weight sharing; pooling for translation invariance.": "Core Concepts / Methods: 1) CNNs (Convolutional Neural Networks) - Mechanism: Convolutions exploit spatial locality and weight sharing; pooling for translation invariance.",
  "Computer Vision: CNNs and Vision Transformers (ViTs) have achieved remarkable results in image classification, object detection, and segmentation.": "Computer Vision: CNNs and Vision Transformers (ViTs) have achieved remarkable results in image classification, object detection, and segmentation.",
  "Introduction: Vision models have evolved from handcrafted features (SIFT/HOG) to deep CNNs and Transformers.": "Introduction: Vision models have evolved from handcrafted features (SIFT/HOG) to deep CNNs and Transformers.",
  "2) Segmentation - Semantic segmentation: Assign class to each pixel (e.g., DeepLab, FCN).": "2) Segmentation - Semantic segmentation: Assign class to each pixel (e.g., DeepLab, FCN).",
  "Title: Computer Vision — Detection, Segmentation, YOLO, and Vision Transformers (ViT) Abstract: This document surveys core computer vision tasks and architectures: object detection, semantic/instance segmentation, the YOLO family, and Vision Transformers (ViT).": "Title: Computer Vision — Detection, Segmentation, YOLO, and Vision Transformers (ViT) Abstract: This document surveys core computer vision tasks and architectures: object detection, semantic/instance segmentation, the YOLO family, and Vision Transformers (ViT).",
  "Core Concepts / Methods: 1) Object Detection - Two-stage: R-CNN, Fast/Faster R-CNN (region proposals then classification/regression).": "Core Concepts / Methods: 1) Object Detection - Two-stage: R-CNN, Fast/Faster R-CNN (region proposals then classification/regression).",
  "4) Vision Transformers (ViT) - Mechanism: Split image into patches; apply transformer encoder with self-attention.": "4) Vision Transformers (ViT) - Mechanism: Split image into patches; apply transformer encoder with self-attention.",
  "- Hybrids: CNN backbones with attention blocks; ConvNeXt as CNN refinement inspired by transformers.": "- Hybrids: CNN backbones with attention blocks; ConvNeXt as CNN refinement inspired by transformers.",
  "Title: Advances in Neural Networks and Deep Learning Abstract: Neural networks have revolutionized the field of artificial intelligence, enabling breakthroughs in computer vision, natural language processing, and reinforcement learning.": "Title: Advances in Neural Networks and Deep Learning Abstract: Neural networks have revolutionized the field of artificial intelligence, enabling breakthroughs in computer vision, natural language processing, and reinforcement learning.",
  "Title: Deep Learning Architectures — CNNs, RNNs, LSTMs, and Transformers Abstract: This document compares major deep learning families—Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs), and Transformers.": "Title: Deep Learning Architectures — CNNs, RNNs, LSTMs, and Transformers Abstract: This document compares major deep learning families—Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs), and Transformers.",
  "Segmentation Techniques: 1) Semantic Segmentation - Architectures: U-Net, DeepLab (atrous convs), PSPNet (pyramid pooling), HRNet (high-resolution branch).": "Segmentation Techniques: 1) Semantic Segmentation - Architectures: U-Net, DeepLab (atrous convs), PSPNet (pyramid pooling), HRNet (high-resolution branch).",
  "- Use Cases: Image classification, detection backbones, segmentation encoders.": "- Use Cases: Image classification, detection backbones, segmentation encoders.",
  "Transformers: The transformer architecture, introduced in \"Attention Is All You Need,\" has become the foundation for many state-of-the-art models in natural language processing.": "Transformers: The transformer architecture, introduced in \"Attention Is All You Need,\" has become the foundation for many state-of-the-art models in natural language processing.",
  "Natural Language Processing: Models like BERT, GPT, and T5 have demonstrated human-level performance on various language understanding and generation tasks.": "Natural Language Processing: Models like BERT, GPT, and T5 have demonstrated human-level performance on various language understanding and generation tasks.",
  "5) Retrieval-Augmented Generation (RAG) - Pipeline: Retrieve relevant passages via dense embeddings; ground generation on retrieved context.": "5) Retrieval-Augmented Generation (RAG) - Pipeline: Retrieve relevant passages via dense embeddings; ground generation on retrieved context; Evaluation: Measure retrieval recall@K and downstream answer faithfulness; log provenance of sources; Cross-lingual Transfer: Zero-shot transfer from high-resource languages to low-re resource ones; QA: Exact Match and F1",
  "Generation (RAG) - Pipeline: Retrieve relevant passages via dense embeddings; ground generation on retrieved context.": "Generation (RAG) - Pipeline: Retrieve relevant passages via dense embeddings; ground generation on retrieved context.",
  "- Evaluation: Measure retrieval recall@K and downstream answer faithfulness; log provenance of sources.": "- Evaluation: Measure retrieval recall@K and downstream answer faithfulness; log provenance of sources.",
  "- Cross-lingual Transfer: Zero-shot transfer from high-resource languages to low-resource ones.": "- Cross-lingual Transfer: Zero-shot transfer from high-resource languages to low-resource ones.",
  "- QA: Exact Match and F1 for extractive; faithfulness metrics for generative answers.": "- QA: Exact Match and F1 for extractive; faithfulness metrics for generative answers.",
  "3) Fine-tuning and Parameter-Efficient Tuning (PEFT) - Full Fine-tuning: Highest capacity but compute- and memory-intensive.": "3) Fine-tuning and Parameter-Efficient Tuning (PEFT) - Full Fine-tuning: Highest capacity but compute- and memory-intensive.",
  "5) Memory–Speed Trade-offs - Flat exact search: Highest memory and accuracy; poor scalability beyond tens of millions.": "5) Memory–Speed Trade-offs - Flat exact search: Highest memory and accuracy; poor scalability beyond tens of millions.",
  "- Composites: IVF-PQ, IVF-HNSW-PQ, or pretransform (OPQ) + IVF-PQ; select per memory and latency budget.": "- Composites: IVF-PQ, IVF-HNSW-PQ, or pretransform (OPQ) + IVF-PQ; select per memory and latency budget.",
  "Conclusion: FAISS provides a powerful and flexible framework for similarity search that can be adapted to a wide range of applications.": "Conclusion: FAISS provides a powerful and flexible framework for similarity search that can be adapted to a wide range of applications.",
  "- Multi-Stage: Two-level retrieval—HNSW/IVF-PQ for candidates, then rerank via exact or cross-encoder.": "- Multi-Stage: Two-level retrieval—HNSW/IVF-PQ for candidates, then rerank via exact or cross-encoder.",
  "- Usage: Flexible; cast many tasks into text-to-text format (translation, QA, summarization).": "- - Usage: Flexible; cast many tasks into text-to-text format (translation, QA, summarization).",
  "PR-AUC, ROC-AUC, precision@K; use PR curves for extreme imbalance.": "3) F1 Score - Harmonic mean of precision and recall; balances both errors. 4) Trade-off: Adjustable via decision threshold; higher precision often lowers recall and vice versa.",
  "3) F1 Score - Harmonic mean of precision and recall; balances both errors.": "3) F1 Score - Harmonic mean of precision and recall; balances both errors.",
  "- Trade-off: Adjustable via decision threshold; higher precision often lowers recall and vice versa.": "- Trade-off: Adjustable via decision threshold; higher precision often lowers recall and vice versa.",
  "- Imbalance: Use class weighting, focal loss, or resampling; evaluate with PR curves and per-class metrics.": "- Imbalance: Use class weighting, focal loss, or resampling; evaluate with PR curves and per-class metrics.",
  "Re-ranking - Candidate Generation: High-recall, low-latency retrieval of hundreds of items (ANN over item vectors, popularity priors).": "Re-ranking - Candidate Generation: High-recall, low-latency retrieval of hundreds of items (ANN over item vectors, popularity priors). Summarization/Generation: ROUGE, BLEU, BERTScore, and human evaluation for coherence and factuality. Usage: Generative tasks (dialog, code, summarization), in",
  "- Summarization/Generation: ROUGE, BLEU, BERTScore, and human evaluation for coherence and factuality.": "- Summarization/Generation: ROUGE, BLEU, BERTScore, and human evaluation for coherence and factuality.",
  "- Usage: Generative tasks (dialog, code, summarization), in-context learning and few-shot prompting.": "- Usage: Generative tasks (dialog, code, summarization), in-context learning and few-shot prompting.",
  "5) T5 (Encoder–Decoder) - Pretraining: Text-to-text objective (span corruption and generation).": "5) T5 (Encoder–Decoder) - Pretraining: Text-to-text objective (span corruption and generation).",
  "Advanced Topics and Practical Guidance: 1) Two-Tower and Sequence Models - Two-Tower: Separate user and item encoders produce embeddings; dot-product for retrieval at scale via ANN (e.g., FAISS, ScaNN).": "Advanced Topics and Practical Guidance: 1) Two-Tower and Sequence Models - Two-Tower: Separate user and item encoders produce embeddings; dot-product for retrieval at scale via ANN (e.g., FAISS, ScaNN).",
  "- Strengths: Strong generative fluency; scalable with data/compute.": "- Strengths: Strong generative fluency; scalable with data/compute."
}